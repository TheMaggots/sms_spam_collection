{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam_Collection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4ppmhtO1AloNHnBeLIcmT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memetics19/Sms_spam_collection-/blob/master/SMS_Spam_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-pLpC9gQvSO",
        "colab_type": "text"
      },
      "source": [
        "# Importing all the libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cTFB4zxyyT",
        "colab_type": "code",
        "outputId": "70ed2f07-deb5-4cb6-fdfc-60acfff723ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import pandas as pd \n",
        "import string \n",
        "import re\n",
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 458
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkKjW0ZcePOs",
        "colab_type": "code",
        "outputId": "ac24e972-6b42-4c99-991c-1673ca555e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "doc = open(\"SMSSpamCollection\").read()\n",
        "doc[0:500]"
      ],
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\nham\\tOk lar... Joking wif u oni...\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tU dun say so early hor... U c already then say...\\nham\\tNah I don't think he goes to usf, he lives around here though\\nspam\\tFreeMsg Hey there darling it's been 3 week's now and no word bac\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 459
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKarzIXFpxt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsed_data=doc.replace('\\t','\\n').split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duMABu5EqS-y",
        "colab_type": "code",
        "outputId": "a4c2139c-d6f3-4f2d-a13d-30ffeb82ca51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "label_list=parsed_data[0::2]\n",
        "msg_list=parsed_data[1::2]\n",
        "print(label_list[0:5])\n",
        "print(msg_list[0:5])"
      ],
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham', 'ham', 'spam', 'ham', 'ham']\n",
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4AmRp8dzLSQ",
        "colab_type": "text"
      },
      "source": [
        "# Read the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Md1-5Eyudca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame({\n",
        "    'label':label_list[:-1],\n",
        "    'sms':msg_list\n",
        "    \n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iivTodYuv49",
        "colab_type": "code",
        "outputId": "4843283f-bd16-40fa-af73-1029bf70501e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 463
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Uvii3Av2Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",header=None)\n",
        "data.columns=['label','sms']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZCaj7dc3qA3",
        "colab_type": "code",
        "outputId": "645408ac-8924-4ea0-ea0f-83687398b77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 465
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjw2JlOzXdp",
        "colab_type": "text"
      },
      "source": [
        "# Shape of the Datasets \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpLpeKUhx3II",
        "colab_type": "code",
        "outputId": "a06d3262-d71a-4bc5-af2f-8ae32ef25bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"input data as {len(data)} rows and {len(data.columns)} columns\")"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data as 5572 rows and 2 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jYxvNvszvTn",
        "colab_type": "text"
      },
      "source": [
        "# Length of the ham and spam "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8QD0O9ozuXA",
        "colab_type": "code",
        "outputId": "2abca662-82cc-458a-953c-0e6e8ac31949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'ham= {len(data[data[\"label\"]==\"ham\"])}')"
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham= 4825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4e7IrVK3yCJ",
        "colab_type": "code",
        "outputId": "73fde305-ef73-4fa1-f53f-68880ca84c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'spam= {len(data[data[\"label\"]==\"spam\"])}')"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam= 747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL_TKiKl4H8q",
        "colab_type": "text"
      },
      "source": [
        "Let's check for missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdExRnQd33j7",
        "colab_type": "code",
        "outputId": "36fcf74a-ade1-474d-c861-63697007fcd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(f\"number of missing label = {data['label'].isnull().sum()}\")\n",
        "print(f\"number of missing messages  = {data['sms'].isnull().sum()}\")"
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing label = 0\n",
            "number of missing messages  = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcgC4DpCG_VW",
        "colab_type": "text"
      },
      "source": [
        "# Removing the Punctuation \n",
        "Punctuation is present in the string library. Let's see the punctuations present in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08d0accMHHsZ",
        "colab_type": "code",
        "outputId": "5ccb5444-5a35-47f8-a600-4fdf422aa901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 470
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvgL3J9KGxs",
        "colab_type": "text"
      },
      "source": [
        "using list comprehension feature removing the punctuation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLlSbcMRJ4Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_punctate(txt):\n",
        "  txt_nopunt=\"\".join([ c for c in txt if c not in string.punctuation])\n",
        "  return txt_nopunt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwjKggbeKr67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['sms_clean']=data['sms'].apply(lambda x: rem_punctate(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjC7QXrxLgoL",
        "colab_type": "code",
        "outputId": "dc73d1df-f2de-4b7b-cc7c-b49632ec9090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 473,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          sms_clean\n",
              "0   ham  ...  Go until jurong point crazy Available only in ...\n",
              "1   ham  ...                            Ok lar Joking wif u oni\n",
              "2  spam  ...  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  ...        U dun say so early hor U c already then say\n",
              "4   ham  ...  Nah I dont think he goes to usf he lives aroun...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5v7uAHBNGZP",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization \n",
        "splitting text into list of words \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8UAlNjNE2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(txt):\n",
        "  tokens=re.split(\"\\W+\",txt)# \"W+\" is represented for non words and \"+\" represents one or more words\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjq7Gm0NOe5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"sms_clean_tokenized\"]=data[\"sms_clean\"].apply(lambda x:tokenize(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2YemZfO0i-",
        "colab_type": "code",
        "outputId": "f13bc95b-25eb-43b0-995c-2419f5ad5974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                sms_clean_tokenized\n",
              "0   ham  ...  [Go, until, jurong, point, crazy, Available, o...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [U, dun, say, so, early, hor, U, c, already, t...\n",
              "4   ham  ...  [Nah, I, dont, think, he, goes, to, usf, he, l...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytxb3RWeP9dc",
        "colab_type": "text"
      },
      "source": [
        "# Removing the stop words\n",
        "\n",
        "stop words are the words are \"is\",\"the\",\"am\".. etc...\n",
        "\n",
        "It should be removed because it will make the data less which is feasible for machine learning model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4OfYX5YP8d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMgCZ1pxR7SH",
        "colab_type": "text"
      },
      "source": [
        "let's see 10 stop words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJmft7icRk0S",
        "colab_type": "code",
        "outputId": "2280e003-8346-4f8e-e252-952e03a85a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_words[0:10]"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 478
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkhadSP1SFKz",
        "colab_type": "text"
      },
      "source": [
        "let's see all the stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdOHsFomSERD",
        "colab_type": "code",
        "outputId": "b1ed36fa-da90-4473-e0fb-9472e7209366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stop_words"
      ],
      "execution_count": 479,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 479
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huXZ5FnmSO9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_stopwords(txt_tokenized):\n",
        "  txt_clean=[word for word in txt_tokenized if word not in stop_words]\n",
        "  return txt_clean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyJ6LLlsWDyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"sms_no_sw\"]=data[\"sms_clean_tokenized\"].apply(lambda x: rem_stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa7T15VzaNym",
        "colab_type": "code",
        "outputId": "206444c5-8ef6-4be5-cc40-c74221d0ba55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          sms_no_sw\n",
              "0   ham  ...  [Go, jurong, point, crazy, Available, bugis, n...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
              "3   ham  ...      [U, dun, say, early, hor, U, c, already, say]\n",
              "4   ham  ...  [Nah, I, dont, think, goes, usf, lives, around...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSWsEWaoh4Rx",
        "colab_type": "text"
      },
      "source": [
        "#Stemming\n",
        "processing into root word by using porter stemmization algorithm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoQq4D4Watp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps=PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFCGhx9Bns5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(tokenized_text):\n",
        "  text=[ps.stem(word) for word in tokenized_text]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jrjlYT5sOx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"stemmed_data\"]=data[\"sms_no_sw\"].apply(lambda x: stemming(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Q9p6qqsmgK",
        "colab_type": "code",
        "outputId": "940ab32b-791b-43c4-9fef-573acae0f5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "      <th>stemmed_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "      <td>[Go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "      <td>[nah, I, dont, think, goe, usf, live, around, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                       stemmed_data\n",
              "0   ham  ...  [Go, jurong, point, crazi, avail, bugi, n, gre...\n",
              "1   ham  ...                       [Ok, lar, joke, wif, u, oni]\n",
              "2  spam  ...  [free, entri, 2, wkli, comp, win, FA, cup, fin...\n",
              "3   ham  ...      [U, dun, say, earli, hor, U, c, alreadi, say]\n",
              "4   ham  ...  [nah, I, dont, think, goe, usf, live, around, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 486
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfOnnNmCk2aC",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization\n",
        " using WordNet Lemmatization algorithm \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vu8b8L9k05v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn=nltk.WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r53Ue45sLsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatization(token_txt):\n",
        "  text=[wn.lemmatize(word) for word in token_txt]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO71Q91puXwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['sms_tokenized']=data['sms_no_sw'].apply(lambda x:lemmatization(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8KOwyA7unzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "7133f84b-b5b3-4b02-d9ff-2145ff6841e5"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 490,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "      <th>stemmed_data</th>\n",
              "      <th>sms_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "      <td>[Go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "      <td>[nah, I, dont, think, goe, usf, live, around, ...</td>\n",
              "      <td>[Nah, I, dont, think, go, usf, life, around, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                      sms_tokenized\n",
              "0   ham  ...  [Go, jurong, point, crazy, Available, bugis, n...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
              "3   ham  ...      [U, dun, say, early, hor, U, c, already, say]\n",
              "4   ham  ...  [Nah, I, dont, think, go, usf, life, around, t...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 490
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdt4_VG7v678",
        "colab_type": "text"
      },
      "source": [
        "#Vectorization\n",
        "using CountVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjhLYvRv6H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv=CountVectorizer(analyzer=lemmatization)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLUMzMriEDOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc23337d-3a63-4ee9-add6-8794d15d1fa8"
      },
      "source": [
        "X = cv.fit_transform(data[\"sms_tokenized\"])\n",
        "print(X.shape)"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5572, 11043)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwkCFVj_KCuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sample=data[0:10]\n",
        "cv1=CountVectorizer(analyzer=lemmatization)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu-GWLHpKPmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139cc532-fa26-4842-b6a3-2495cb0cbbe5"
      },
      "source": [
        "X=cv1.fit_transform(data_sample[\"sms_tokenized\"])\n",
        "print(X.shape)"
      ],
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGQ5QeJVKwsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame(X.toarray(), columns=cv1.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqLXLhuFK_Jw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "f4f7b583-9250-44ab-c151-601e625f21d8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>08002986030</th>\n",
              "      <th>08452810075over18s</th>\n",
              "      <th>09061701461</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>150</th>\n",
              "      <th>2</th>\n",
              "      <th>2005</th>\n",
              "      <th>21st</th>\n",
              "      <th>3</th>\n",
              "      <th>87121</th>\n",
              "      <th>9</th>\n",
              "      <th>900</th>\n",
              "      <th>As</th>\n",
              "      <th>Available</th>\n",
              "      <th>Call</th>\n",
              "      <th>Callers</th>\n",
              "      <th>Callertune</th>\n",
              "      <th>Cine</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Co</th>\n",
              "      <th>Cup</th>\n",
              "      <th>Even</th>\n",
              "      <th>FA</th>\n",
              "      <th>FREE</th>\n",
              "      <th>Free</th>\n",
              "      <th>FreeMsg</th>\n",
              "      <th>Go</th>\n",
              "      <th>Had</th>\n",
              "      <th>Hey</th>\n",
              "      <th>I</th>\n",
              "      <th>Id</th>\n",
              "      <th>Joking</th>\n",
              "      <th>KL341</th>\n",
              "      <th>May</th>\n",
              "      <th>Melle</th>\n",
              "      <th>Minnaminunginte</th>\n",
              "      <th>Mobile</th>\n",
              "      <th>Nah</th>\n",
              "      <th>Nurungu</th>\n",
              "      <th>...</th>\n",
              "      <th>like</th>\n",
              "      <th>mobile</th>\n",
              "      <th>month</th>\n",
              "      <th>n</th>\n",
              "      <th>network</th>\n",
              "      <th>ok</th>\n",
              "      <th>oni</th>\n",
              "      <th>patent</th>\n",
              "      <th>per</th>\n",
              "      <th>point</th>\n",
              "      <th>prize</th>\n",
              "      <th>questionstd</th>\n",
              "      <th>rateTCs</th>\n",
              "      <th>rcv</th>\n",
              "      <th>receive</th>\n",
              "      <th>receivea</th>\n",
              "      <th>request</th>\n",
              "      <th>reward</th>\n",
              "      <th>say</th>\n",
              "      <th>selected</th>\n",
              "      <th>send</th>\n",
              "      <th>set</th>\n",
              "      <th>speak</th>\n",
              "      <th>std</th>\n",
              "      <th>still</th>\n",
              "      <th>think</th>\n",
              "      <th>though</th>\n",
              "      <th>tkts</th>\n",
              "      <th>treat</th>\n",
              "      <th>txt</th>\n",
              "      <th>u</th>\n",
              "      <th>usf</th>\n",
              "      <th>valued</th>\n",
              "      <th>wat</th>\n",
              "      <th>week</th>\n",
              "      <th>wif</th>\n",
              "      <th>win</th>\n",
              "      <th>wkly</th>\n",
              "      <th>word</th>\n",
              "      <th>world</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  136 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   08002986030  08452810075over18s  09061701461  11  ...  win  wkly  word  world\n",
              "0            0                   0            0   0  ...    0     0     0      1\n",
              "1            0                   0            0   0  ...    0     0     0      0\n",
              "2            0                   1            0   0  ...    1     1     0      0\n",
              "3            0                   0            0   0  ...    0     0     0      0\n",
              "4            0                   0            0   0  ...    0     0     0      0\n",
              "\n",
              "[5 rows x 136 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 497
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVClKGALwVB",
        "colab_type": "text"
      },
      "source": [
        "using N-Grams Vectorization algorithm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxKfjtrtLsRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}